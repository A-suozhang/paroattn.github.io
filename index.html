<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models">
  <meta name="keywords" content="Diffusion Transformer, Quantization, Efficent Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    .callout {
      background-color: #f0f7ff;
      border-left: 4px solid #0077ff;
      padding: 1.5rem;
      margin: 1.5rem 0;
      border-radius: 4px;
    }
    .callout-title {
      color: #0077ff;
      font-weight: bold;
      margin-bottom: 0.5rem;
      font-size: 1.5rem;
    }
    .callout-image {
      display: block;
      margin: 1rem auto;
      width: 90%;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title"> <img src="./static/images/paro_icon.svg" style="width: 4em" /> <span style="color: #0077ff">PARO</span>Attention: <span style="color: #0077ff">P</span>attern-<span style="color: #0077ff">A</span>ware <span style="color: #0077ff">R</span>e<span style="color: #0077ff">O</span>rdering for Efficient Sparse and Quantized Attention<br>in Visual Generation Models</h1>
          <!-- <h3 class="title is-4 publication-title">ICLR2025</h3> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://nicsefc.ee.tsinghua.edu.cn/people/TianchenZhao">Tianchen Zhao</a><sup>1,2*</sup>,</span>
            <span class="author-block">
              <a href="https://nicsefc.ee.tsinghua.edu.cn/people/KeHong">Ke Hong</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://nicsefc.ee.tsinghua.edu.cn/people/XinhaoYang">Xinhao Yang</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Xuefeng_Xiao1">Xuefeng Xiao</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Huixia_Li2">Huixia Li</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Feng_Ling1">Feng Ling</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://nicsefc.ee.tsinghua.edu.cn/">Ruiqi Xie</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://nicsefc.ee.tsinghua.edu.cn/">Siqi Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://nicsefc.ee.tsinghua.edu.cn/">Hongyu Zhu</a><sup>1</sup>,</span>
            <span class="author-block">
              <span class="author-block">
                <a href="https://nicsefc.ee.tsinghua.edu.cn/">Yichong Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://nicsefc.ee.tsinghua.edu.cn/people/YuWang">Yu Wang</a><sup>1,&dagger;</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>Bytedance Seed</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>&dagger;</sup><b>Corresponding author</b></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.16054"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2506.16054"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Sides Link. -->
              <!-- <span class="link-block">
                <a href="https://nicsefc.ee.tsinghua.edu.cn/%2Fnics_file%2Fpdf%2Fa3462020-25d8-436c-993f-7aaf047cbd93.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-alt"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/thu-nics"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">

  <div class="container is-max-desktop">
    <!--/ Poster. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Poster</h2>
        <div class="publication-poster">
          <img src="./static/src/poster.png"
            class="poster"
            lt="Poster."/> -->
            <!-- <embed src="./static/src/poster.pdf" width="1600px" height="900px" /> -->
        <!-- </div>
      </div>
    </div> 
    <!--/ Video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Video</h2>
        <div class="publication-poster">
          <iframe width="1080" height="720" src="https://www.youtube.com/embed/N_llpMqMJbk?si=C38fAW1gYxCQFtLc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
            <!-- <embed src="./static/src/poster.pdf" width="1600px" height="900px" /> -->
        <!-- </div>
      </div>
    </div> 
    <!--/ Teaser. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="callout">
          <div class="callout-title">TL;DR</div>
          <div class="content has-text-justified">
            <ul style="list-style-type: none; padding-left: 0;">
              <li style="margin-bottom: 1rem;">
                We propose <b>PAROAttention:</b> A simple yet effective approach to enhance the efficiency of attention in visual generative models. By taking a novel alternative approach of "reorganize" the attention pattern through <b>"Pattern-Aware token ReOrder"</b>, it simultaneously improves both the attention **sparsification and quantization**, achieving superior performance preservation and hardware efficiency. PAROAttention achieves <b>20% density (5x sparse)</b>, and <b>INT4 quantization</b> of both QK and PV computing on mainstream video (CogVideo, Wan) and image generation (Flux) models, speedup attention computing by <b>3-10x</b> and <b>2-4x</b> end-to-end acceleration while maintaining generation quality.
              </li>
              <video autoplay controls muted loop playsinline style="width: 45%; margin: 2rem auto 1rem auto; display: block;">
                <source src="./static/src/paroattn/paro_reorder.mp4" type="video/mp4">
              </video>
            </ul>
          </div>
        </div>
      </div>
    </div>

    <!-- Teaser image moved outside callout -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <img src="./static/src/paroattn/paro_teaser.png"
        class="teaser"
        alt="Teaser."
        style="width: 90%"
        />
      </div>
    </div>

    <!-- Additional content moved outside callout -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p style="margin-bottom: 1rem;">
            The remaining content further elaborates the idea of PAROAttention as follows:
          </p>
          <ul style="list-style-type: disc; padding-left: 2rem; margin-bottom: 1rem;">
            <li style="margin-bottom: 1rem;">
              We analyze <b>key challenges</b> in attention sparsity and low-bit quantization for visual generation models, identifying their common issue as <b>"diverse and scattered"</b> attention patterns.
            </li>
            <li style="margin-bottom: 1rem;">
              Based on the <b><span style="color:#b3280d">"locality"</span></b> prior in visual feature extraction, we show that these patterns arise from the tokenization process, which disrupts 3D spatial adjacency. The diverse patterns actually describe local aggregations in 3D space, which can be <b>unified into a "regular and block-based" attention pattern</b>.
            </li>
            <li style="margin-bottom: 1rem;">
              We design a simple <b>"Token Reorder"</b> method to convert diverse attention patterns into a unified, hardware-friendly block pattern. Tailored sparse and quantization strategies are introduced to optimize performance and hardware efficiency. Through CUDA Kernel implementation, PAROAttention achieves <b>20% densification</b> with <b>INT4 quantization</b> on mainstream video (CogVideo, Wan) and image generation (Flux) models, improving attention and end-to-end acceleration while maintaining generation quality.
            </li>
          </ul>
        </div>
      </div>
    </div>

    <head>
        <!-- <meta charset="UTF-8"> -->
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Video and Image Demos</title>
        <style>
            /* 通用样式 */
            h2 {
                text-align: center;
                margin-bottom: 20px;
                margin-top: 20px; 
            }

            /* 视频网格样式 */
          .video-grid {
                display: grid;
                grid-template-columns: repeat(2, 1fr);
                gap: 20px;
                justify-items: center;
                margin-bottom: 40px;
            }

          .video {
                width: 100%;
            }

          .video video {
                width: 100%;
                height: auto;
            }

          .caption {
                text-align: center;
                margin-top: 10px;
                font-weight: bold;
            }

            /* 图片网格样式 */
          .image-grid {
                display: grid;
                grid-template-columns: repeat(4, 1fr);
                gap: 20px;
                justify-items: center;
            }

          .image-grid img {
                width: 100%;
                height: auto;
                object-fit: cover;
            }

          .image-grid figure {
                margin: 0;
            }

          .image-grid figcaption {
                text-align: center;
                font-size: 0.9em;
                color: #333;
                margin-top: 10px;
            }

          .image-item {
                text-align: center;
          }

          .image-caption {
                text-align: center;
                margin-top: 10px;
                font-weight: bold;
                font-size: 0.9em;
                color: #333;
            }
        </style>
    </head>

    <!-- 视频展示部分 -->
    <h2 class="title is-3">Generation Quality</h2>
    <div class="video-grid">
        <!-- 第一个视频 -->
        <div class="video">
            <video autoplay controls muted loop playsinline>
                <source src="static/src/paroattn/FP.mp4" type="video/mp4">
            </video>
            <div class="caption">
                <b>FP16 Baseline</b><br>
                1x Speedup.
            </div>
        </div>
        <!-- 第二个视频 -->
        <div class="video">
            <video autoplay controls muted loop playsinline>
                <source src="static/src/paroattn/paro_attn_0.3_int8.mp4" type="video/mp4">
            </video>
            <div class="caption">
              <b>PAROAttn</b> <span style="font-size: 0.9em;"> (30% Sparse + INT8)</span><br>
              <b><span style="color:#b3280d">5.72x</span></b> Speedup.
            </div>
        </div>
        <!-- 第三个视频 -->
        <div class="video">
            <video autoplay controls muted loop playsinline>
                <source src="static/src/paroattn/sparge_0.5.mp4" type="video/mp4">
            </video>
            <div class="caption">
              <b>SpargeAttn</b> <span style="font-size: 0.9em;"> (50% Sparse)</span><br>
              <b><span style="color:#b3280d">1.67x</span></b> Speedup.
            </div>
        </div>
        <!-- 第四个视频 -->
        <div class="video">
            <video autoplay controls muted loop playsinline>
                <source src="static/src/paroattn/sparse_video_gen_0.3.mp4" type="video/mp4">
            </video>
            <div class="caption">
              <b>SparseVideoGen</b> <span style="font-size: 0.9em;"> (30% Sparse)</span><br>
              <b><span style="color:#b3280d">1.42x</span></b> Speedup. <br>
              <span style="font-size: 0.7em;"> (* without timestep skipping in original implementation)</span><br>
            </div>
        </div>
    </div>

    <!-- 图片展示部分 -->
    <div class="image-grid">
        <div class="image-item">
            <img src="static/src/paroattn/fp16.jpg" alt="Teaser Image 0">
            <div class="image-caption">
                <b>FP16 Baseline</b><br>
                1x Speedup.
            </div>
        </div>
        <div class="image-item">
            <img src="static/src/paroattn/paro_0.5_int4.png" alt="Teaser Image 1">
            <div class="image-caption">
                <b>PAROAttn</b> <span style="font-size: 0.9em;"> (50% Sparse + INT4)</span><br>
                <b><span style="color:#b3280d">7.61x</span></b> Speedup.
            </div>
        </div>
        <div class="image-item">
            <img src="static/src/paroattn/ditfastattn.jpg" alt="Teaser Image 2">
            <div class="image-caption">
              <b>DiTFastAttn</b> <span style="font-size: 0.9em;"> (50% Sparse)</span><br>
              <b><span style="color:#b3280d">1.38x</span></b> Speedup.
            </div>
        </div>
        <div class="image-item">
            <img src="static/src/paroattn/sparge.jpg" alt="Teaser Image 3">
            <div class="image-caption">
              <b>SpargeAttn</b> <span style="font-size: 0.9em;"> (50% Sparse)</span><br>
                <b><span style="color:#b3280d">1.61x</span></b> Speedup.
            </div>
        </div>
    </div>

    <h2 class="title is-3">Hardware Speedup</h2>
    <p>
      We implement efficient CUDA Implementation of PAROAttention, including kernel fusion, prefetching to minimize the overhead. We conduct comparison with current attention sparsification and quantization methods, and demonstrate PAROAttn's superior performance in both hardware efficiency and generation quality.
    </p>
    <img src="./static/src/paroattn/hardware_bar.jpg"
    style="width: 100%; margin-top: 1rem;"
    />
    <br/>
    <br/>
  </div>
</section>


<section class="section">
  <!-- Motivation. -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Preliminary Analysis</h2>
        <h3 class="title is-4">Key Challenge for Attention Sparsification and Quantization</h3>
        <div class="content has-text-justified">
          <p>To find a unified solution for Attention sparsity and quantization, we analyze the key challenges in optimizing Attention efficiency through sparsity and low-bit quantization. We conclude the key challenges arise from the <b><span style="color:#b3280d">diverse and scattered visual attention patterns</span></b> (as shown on the left side of the figure below).</p>
<ul>
  <li><strong>Sparse Attention:</strong> requires consideration of both maintaining algorithmic performance and improving hardware efficiency.
    <ul>
      <li>For <b>algorithmic performance</b>: it is essential to avoid incorrectly removing large values during the sparsification. Due to the diverse structures in visual attention patterns (such as diagonal, vertical, block-wise, etc.), which dynamically change across different time steps and control signals, the diversity and dynamic nature of attention patterns make it difficult to design a mask that can fully capture important values.</li>
      <li>For <b>hardware efficiency</b>, it is necessary to design "structured" sparse masks that allow for the skipping of entire blocks of computation to achieve actual hardware benefits. Arbitrary irregular sparsity requires additional indexing operations and may lead to fragmented computational loads. Specifically, because FlashAttention involves block-wise attention computation, sparsification should also consider how to adapt to this method. Since the attention patterns in visual attention maps often do not correspond to the blocks in FlashAttention (e.g., in diagonal patterns, only a small number of large values exist in each block), the scattered nature of the attention map patterns makes structured sparsity difficult to achieve, limiting the potential for effective hardware efficiency improvement.</li>
    </ul>
  </li>

  <li><strong>Low-Bit Quantization Algorithms</strong>: The key issue lies in minimizing quantization loss.
    <ul>
      <li>Existing work (e.g., <a href="https://a-suozhang.xyz/viditq.github.io/" target="_blank">ViDiT-Q</a>) has already analyzed and identified the primary source of error in low-bit quantization as the <b>"data variation within quantization groups"</b> For attention quantization, it is necessary to select block-wise quantization groups to adapt to FlashAttention. However, the "diagonal" visual attention patterns result in outliers on the diagonal elements within the block-wise quantization groups, causing significant intra-group data differences.</li>
    </ul>
  </li>
</ul>
        </div>
        <div class="content has-text-centered">
          <img src="static/src/paroattn/paro_motiavtion.png"
          class="exp-image"
          alt="Experimental Results Image."
          style="width: 100%"/>
        </div>

        <h3 class="title is-4">How to understand diverse patterns? from the perspective of <span style="color:#b3280d">"Locality"</span></h3>
        <div class="content has-text-justified">
<p>To address the aforementioned challenges, we seek an approach to <b>"reorganize" the attention maps</b> to obtain more uniform and concentrated attention patterns. Inspired by the <b><span style="color:#b3280d">locality prior of visual feature extraction</span></b> (e.g., CNN, SwinTransformer design principles, and Hubel & Wiesel's biological experiments), we further analyze the causes of the diversity of visual attention patterns and identify that <b>"diverse visual attention patterns all describe local aggregation in space."</b></p>

<p>As shown in the figure below, during the tokenization, the original 3D space (F - frame count, H, W - width and height of each frame's image) is reshaped into a 1D token sequence, arranged in the default order of <code>[F, H, W]</code>. This transformation causes pixels in adjacent positions of the 3D space (except for the last dimension (W), which is contiguous in memory) to be spaced out in the token sequence. <b><i>The tokens with the same interval actually describes neighboring pixels along certain dimension in the 3D space.</i></b> Therefore, <b><i>multi-diagonal attention patterns actually describe "local aggregation in other dimensions,"</i></b> and can be transformed into block-wise patterns that represent local aggregation through token sequence reordering (by transforming the local aggregation dimensions into contiguous memory dimensions, such as <code>[F, H, W]</code> -> <code>[F, W, H]</code>).</p>

<p>We further verified that each different attention head (Head) consistently presents local aggregation in a particular dimension in different scenarios. As a result, we can <b>design an appropriate token reordering scheme for each head to transform the diverse and scattered attention patterns into uniform, hardware-friendly block-wise patterns, <span style="color:#b3280d">significantly ease the difficulty the sparsification and quantization of Attention</span></b>. This approach leverages the locality of visual feature extraction on the algorithm side (better numerical locality) and aligns it with the locality of hardware computation (better memory and computation locality), thus achieving both optimal algorithm performance retention and hardware efficiency improvement.</p>

          <div class="content has-text-centered">
            <img src="static/src/paroattn/paro_locality.png"
            class="exp-image"
            alt="Experimental Results Image."
            style="width: 100%"/>
          </div>
        
        </div>



        <br/>
      </div>
    </div>
  </div>
</div>

<section class="section">
  <!-- Motivation. -->
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Methodology</h2>
        <p>
          <p>The flow of our approach is shown in the figure below. For the main bottlenecks in Attention computation, namely the two large-scale matrix multiplications (QK and PV), we have performed both sparsification and quantization optimization, significantly reducing their hardware overhead. We offline determine the token reordering scheme for each attention head (Head) and the corresponding sparse mask based on a small amount of corrective data, which introduces almost no additional overhead during inference. During inference, we only need to skip the attention blocks corresponding to the sparse mask and perform low-bit quantization block-by-block on the remaining parts.</p>

        </p>

        <div class="content has-text-centered">
          <img src="static/src/paroattn/paro_method.png"
          class="exp-image"
          alt="Experimental Results Image."
          style="width: 100%; margin-top: 1rem;"
          />
        </div>

        <h3 class="title is-5">Pattern-Aware Token ReOrder (PARO)</h3>
        <div class="content has-text-justified">
          <p>We have discovered that each different attention head (Head) consistently presents local aggregation in a particular dimension under different scenarios. Therefore, we can offline select an appropriate token reordering method for each attention head to transform the attention map into a block-wise pattern that exhibits local aggregation. We select a special case in the reordering process, (<b><i>dimension permutation</i></b>), which yields good results. For the feature [F, H, W] of video generation models, we explored six possible permutation methods for each attention head and offline selected the optimal permutation to achieve the desired data distribution. Since <b>attention sparsification and quantization require different data distribution properties</b>, we designed selection criteria for reordering methods specifically for sparsification and quantization, combining both as the final criterion.</p>

<ul>
  <li><strong>Sparsification perspective</strong>: To minimize the loss caused by structured sparsity, the goal is to ensure as many blocks as possible are fully sparse (<b><i>Block Sparse</i></b>).</li>
  <li><strong>Quantization perspective</strong>: To reduce quantization loss caused by large intra-block data distribution differences, the goal is to make the data distribution within the block as uniform as possible (<b><i>Block Uniform</i></b>).</li>
</ul>

<p>As shown in the figure below, sparsification and quantization have different distribution requirements for attention maps. We need to combine both requirements to find a reordering method that fits both. After appropriate reordering, the attention map exhibits a block-wise and more concentrated distribution, suitable for both sparsification and quantization processing.</p>

          <div class="content has-text-centered">
            <img src="static/src/paroattn/reorder_vis.png"
            class="exp-image"
            alt="Experimental Results Image."
            style="width: 70%"/>
          </div>

        <h3 class="title is-5">Attention Sparsification</h3>
        <div class="content has-text-justified">
          <p>Existing sparse attention schemes can be divided into two approaches: (1) dynamic sparsification schemes (e.g., SpargeAttention), which generate sparse masks online based on attention values; and (2) static sparsification schemes (e.g., DiTFastAttn), which generate sparse masks offline. Both approaches have their advantages and disadvantages. Although the token reordering scheme (PARO) designed in this paper <b>can help both dynamic and static schemes</b>, we analyze the advantages and disadvantages of both and ultimately select the static sparsification scheme as the main sparsification approach for PAROAttention. The detailed analysis is as follows:</p>

          <ul>
            <li><strong>Dynamic Sparsification (Dynamic Approach)</strong>:
              <ul>
                <li>For performance preservation: although dynamic schemes can naturally adapt to dynamically changing patterns, they generate sparse masks online based on the attention values before the Softmax operation (<span style="color:#b3280d">pre-softmax</span>). These values are relatively uniform and do not exhibit obvious patterns, making it difficult to accurately identify the patterns.</li>
                <li>For hardware efficiency, dynamic sparsification introduces the additional overhead of calculating the sparse mask online. This overhead is a trade-off with the accuracy of the mask prediction. To obtain an accurate mask, a relatively large amount of additional computation must be introduced. This additional prediction process generally requires sophisticated CUDA Kernels to achieve high efficiency gains.</li>
              </ul>
              <p>In summary, <span style="color:#b3280d">under low sparsity ratios, dynamic sparsification faces a bottleneck in terms of both accuracy and efficiency improvement</span>. Therefore, we resort to the static sparsification scheme in this paper.</p>
            </li>
          
            <li><strong>Static Sparsification (Static Approach)</strong>:
              <ul>
                <li>For performance preservation, since static attention maps are difficult to adapt to diverse and dynamically changing attention patterns, static sparsification schemes generally result in more significant performance losses compared to dynamic schemes. However, the attention map reorganization in <b>PAROAttention has transformed the diverse and dynamically changing attention patterns into more regular and uniform patterns</b>, addressing the key challenge of static sparsification. Thus, by utilizing the more distinct <span style="color:#b3280d">post-softmax</span> attention maps, PAROAttention achieves better algorithmic performance retention than the dynamic approach.</li>
                <li>In terms of hardware efficiency, although the additional computation overhead of generating sparse masks online is avoided, offline sparse masks introduce additional memory overhead. To address this issue, optimizations have been made in this paper (see the "CUDA System Design" section below).</li>
              </ul>
              <p>After reordering, the attention map presents a uniform and concentrated block-wise pattern. Therefore, we can offline compute the sum of attention data within each block and design a threshold to determine whether the current block should be skipped (this threshold can be adjusted to control density). The sparse mask can be offline obtained without introducing any additional overhead during inference. As shown in the figure below, compared to other existing static attention sparsification schemes, PAROAttention avoids the complex and restricted mask design due to the pre-unification of attention patterns, allowing for sparse masks that fit well with the original attention map.</p>
            </li>
          </ul>
          
        </div>
        <div class="content has-text-centered">
          <img src="static/src/paroattn/sparse_mask_vis.png"
          class="exp-image"
          alt="Experimental Results Image."
          style="width: 50%"/>
        </div>

        <h3 class="title is-5">Quantization Design</h3>
        <div class="content has-text-justified">
          <p>For low-bit quantization, the key metric for evaluating quantization loss is the data variance within groups. Existing literature typically uses incoherence to measure this, which is defined as the ratio of the maximum value to the mean value within the current data group (<code>x.max() / x.abs().mean()</code>). After appropriate token reordering, the significant data variance within the Attention Map blocks is significantly alleviated, thereby enabling support for lower bit-width quantization.</p>
        </div>
        <div class="content has-text-centered">
          <img src="static/src/paroattn/incoherence.png"
          class="exp-image"
          alt="Experimental Results Image."
          style="width: 70%"/>
        </div>
        <br/>

        <h3 class="title is-5">CUDA System Design</h3>
        <div class="content has-text-justified">
          <p><strong>Minimizing Additional Overhead:</strong> The additional overhead introduced by PAROAttention mainly includes the following two aspects. We have conducted targeted optimizations at the system level to minimize this overhead.</p>

          <ul>
            <li><strong>Online Token Reordering Overhead:</strong> Although the reordering method is determined offline, the token reordering process (dimension permutation) needs to be performed online. To avoid the overhead of moving memory from GPU Global Memory to Shared Memory in a single operation, we performed operator fusion (Layer Fusion) by modifying only the write address order of the operators before reordering. The additional overhead introduced by this operation is negligible.</li>
            <li><strong>Memory Overhead of Static Sparse Masks:</strong> Since the attention map is large, the sparse masks determined offline may occupy gigabytes of GPU memory. To reduce this overhead, we adopted a prefetch strategy by creating a new CUDA stream. During each computation, only the sparse mask of the current layer is read, thus reducing the additional memory overhead to several megabytes.</li>
          </ul>
          
          <p><strong>Compatibility:</strong> Both the sparsification and quantization schemes in PAROAttention are processed block by block, making them directly compatible with FlashAttention. Since both the reordering and sparse mask generation are completed offline, no fine-tuned CUDA kernel optimizations are required. Only support for skipping entire block computations based on FlashAttention is necessary, enabling broad adaptability to various scenarios.</p>
        </div>
        <br/>
                
      </div>
    </div>
  <!--/ Motivation. -->
  </div>
</div>
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3"> Experiments and Analysis  </h2>
          <!-- Perf. -->
          <h3 class="title is-4"> Algorithm Performance Preservation </h3>
          <div class="content has-text-justified">
          <p>This paper tests multiple metrics on mainstream video (CogVideo) and image generation models (Flux), including:</p>
          <ul>
            <li><strong>Video Quality Metrics:</strong> CLIPSIM measures semantic consistency; VQA measures video quality; FlowScore measures temporal consistency.</li>
            <li><strong>Differences from Floating-Point Generation:</strong> FVD-FP16 measures feature space differences, PSNR/CosSim measures pixel space differences, and SSIM measures structural similarity.</li>
          </ul>
          
          <p>The typical experimental conclusions are summarized as follows:</p>
          <ol>
            <li>Other baseline sparse schemes cause noticeable quality loss (including content changes, image blurring, etc.) even at relatively high sparsity ratios (50%). In contrast, PAROAttention's sparsification scheme can generate results that are very similar to floating-point results at a 20% higher sparsity ratio, achieving better overall metrics by 50% compared to the baseline schemes.</li>
            <li>The token reordering scheme, PARO, is not limited to static sparsification schemes. It can directly adapt to dynamic sparsification schemes such as SpargeAttention, improving generation performance. Combining PARO with SpargeAttention at 30% density results in generation quality equivalent to SpargeAttention at 50% density, improving the acceleration ratio from 1.67x to 2.22x.</li>
            <li>Compared to SageAttentionV2 (QK INT4, PV FP8), PAROAttention's quantization scheme can further quantize PV to INT4 without any loss in precision.</li>
            <li>The sparsification and quantization schemes of PAROAttention can be used concurrently. The most aggressive optimization scheme (50% + INT4) achieves nearly 10 times better Attention latency optimization compared to floating-point, while maintaining algorithmic performance similar to the baseline method, which only achieves around 2x latency optimization.</li>
          </ol>
          </div>
          
          <!-- Experimental Results Images -->
          <div class="content has-text-centered">
            <h4 class="title is-5" style="margin-top: 2rem;">Experimental Results: CogVideoX</h4>
            <img src="static/src/paroattn/table_cogvideo.png"
            class="exp-image"
            alt="Quantitative Results"
            style="width: 90%; margin-bottom: 2rem;"
            />
          </div>

          <div class="content has-text-centered">
            <h4 class="title is-5">Qualitative Comparison: CogVideoX</h4>
            <img src="static/src/paroattn/qualitative_cogvideo.png"
            class="exp-image"
            alt="Qualitative Comparison"
            style="width: 85%; margin-bottom: 2rem;"
            />
          </div>

          <div class="content has-text-centered">
            <h4 class="title is-5">Qualitative Comparison: Flux</h4>
            <img src="static/src/paroattn/qualitative_flux.png"
            class="exp-image"
            alt="Ablation Studies"
            style="width: 80%; margin-bottom: 2rem;"
            />
          </div>

          <h3 class="title is-4"> Hardware Efficiency Speedup</h3>
          <div class="content has-text-justified">
            <p>We further analyzes the system-level optimization techniques, with the key experimental conclusions summarized as follows:</p>
<ol>
  <li><strong>PAROAttention's sparsification scheme</strong> achieves both superior algorithm performance retention and efficiency improvement. Taking 50% density as an example, PAROAttention achieves a 1.73x attention speedup, surpassing SpargeAttention (1.67x) and SparseVideoGen (1.42x) under the same conditions. This is because the static sparsification scheme introduces almost no additional overhead, whereas the baseline scheme's online sparse mask generation/selection incurs an extra overhead of about 6% to 10%, which becomes more noticeable at lower densities.</li>
  <li><strong>The speedup ratio of PAROAttention</strong> is close to the theoretical limit (50% density, theoretical 2x, actual 1.73x), highlighting the hardware-friendly nature of the approach.</li>
  <li><strong>The additional overhead</strong> of PAROAttention has been effectively reduced and is controlled within 1% of the total system cost.</li>
</ol>
<div class="content has-text-centered">
  <h4 class="title is-5">Latency Speedup</h4>
  <div class="columns is-centered">
    <div class="column is-6">
      <img src="static/src/paroattn/speedup_1.png"
      class="exp-image"
      alt="Latency Speedup 1"
      style="width: 100%;"
      />
    </div>
    <div class="column is-5">
      <img src="static/src/paroattn/speedup_2.png"
      class="exp-image"
      alt="Latency Speedup 2"
      style="width: 100%;"
      />
    </div>
  </div>
</div>
</div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Future Insights</h2>
    <div class="content has-text-justified">
      <p>In summary, this paper focuses on the <b><span style="color:#b3280d">"locality"</span></b> characteristics of visual generation tasks. Through a simple yet effective token reordering operation, we can simultaneously achieve better algorithm performance retention by leveraging the <b>locality of visual feature extraction (better numerical locality) and align it with the locality of hardware computation (better memory and computation locality)</b>, thereby obtaining improvements in both algorithm performance and hardware efficiency. The PAROAttention approach is primarily designed to optimize inference efficiency; however, the concept of utilizing token reordering to better exploit feature extraction locality is <b>beyond inference optimization</b>. Different attention heads autonomously learn local aggregation in different dimensions, which can inspire optimizations in training methods, image parameterization, the design of three-dimensional positional encodings, and further advance the construction of visual backbone models with reasonable inductive biases.</p>

      </div>
  </div>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhao2024viditq,
      title={ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation}, 
      author={Tianchen Zhao and Tongcheng Fang and Enshu Liu and Wan Rui and Widyadewi Soedarmadji and Shiyao Li and Zinan Lin and Guohao Dai and Shengen Yan and Huazhong Yang and Xuefei Ning and Yu Wang},
      year={2024},
      eprint={2406.02540},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
    }</code></pre>
  </div>

  <!-- <div class="content has-text-centered">
    <img src="./static/src/pr.png"
    class="geo-vis"
    alt="Attention map visualization."/>
  </div> -->
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2307.08209">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/A-suozhang/ada3d" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
